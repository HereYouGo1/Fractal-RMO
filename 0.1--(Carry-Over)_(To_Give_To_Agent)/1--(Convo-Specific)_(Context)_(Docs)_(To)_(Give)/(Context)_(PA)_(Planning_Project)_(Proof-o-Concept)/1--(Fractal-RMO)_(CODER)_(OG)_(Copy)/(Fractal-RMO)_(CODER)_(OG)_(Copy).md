Break down coding into small parts **whenever coding something follow X thought process (general one that ST metaprompt first, creates a meta prompt to feed itself about the optimal process (no need to think through it yourself I mean, just prompt metaprompt to create you a prompt regarding finding what is optimal at that smaller scale based on LLM limitations and what they work best unser, then give that to metaprompt to create optimal instructions for CODER to follow (then get it to create CODER synergistically with those instructions that use ST instructions to code each individual part**…..While making CODER could say something along the lines of the necessity for a sequential thinking structure and thus workflow that optimizes the coder by reverse engineering the actual process LLMs deep down (by having a thorough understanding of how transformers work, how vectors and vector databases work, how machine learning and neural networks work, you can ensure the CODER Prompt agent is et up in such a way that it overcomes all of its own LLM issues that typically cause code errors  (***You can deep research all these and make documents for Meta-Prompt to work with as supplementary documents***). This way the CODER works within its limitations, and is proactive about overcoming these limitations by having it coded (or natural language coded) into the prompt what is needed in order to do stuff in a particular order. As it codes, it can write all its failures ect into a separate failure log. This failure log can be used by an OPTIMIZER submodule another prompt agent that is specifically set up to view and analyze the log in order to find patterns of problems and categorize them and identify not only that they are happening, but with a deep understanding about how LLMs are structured, identify what is causing the problem. He puts this all into his own log that has again, (categories of problems, adds instances to categories explaining the situation, explain cause or combination of causes 	-HOLY SHIT —> THE SECRET TO ALL OF THIS IS THE LOGS!!! —> If you make these logs then it give you the missing piece of the puzzle here - DATA. These logs will be a blueprint for what LLMs fuck up on coding wise. Logs can be categorized into specific phases of development to narrow the scope. Not only this but you can have yet another OPTIMIZER submodule that creates training modules. The training modules will be tasks for the CODER to run through, SPECIFICALLY designed to extract the issues he (and Opus 4.1 under him) have regarding outputting perfect code….. This process will give you exactly what you need in order to get the data you need to shine a light on to WHAT actually needs to be designed to fill LLM gaps when it comes to coding, and even simply deeper gaps in them in general and how they can be improved. The OPTIMIZER’s training and logs don’t need to stop at coding either. It can dominate every single sector. Language output —> Logs, training modules, “SHAKESPEARE” agent update….. Do this for EVERY aspect that LLMs fail in. Bias, thinking patterns, user feedback optimization (if indicated its necessary). What comes out is not only an INSANE amount of data on what these LLMs need allowing for optimized training modules to which you can optimize LLM training, and train your own LLM to be superior to any on the market, but it also gives you prompt agents that actually do these things, and in combination, a system prompt that actually fixes LLMs as they are. Now, this system prompt would be far too long for any LLM other than Gemini, BUT, you could set up a UI that connects user to the API of an LLM, have these locked and loaded, then, the orchestrator agent simply needs to use the LLM API to determine what stack of prompt agents is required for complete the prompt, and what the optimal combination of this is. -Could tie in to TaskMaster such that the CODER is your coding agent coding the entire process (all tasks and subtasks from a folder of tasks)	-Could be instructed to do so sequentially, as well as the tasks come.		-Every task could put out specific tests that AI could run themselves (project would need a look over to get anything such as API keys and anything else needed from humans to automate this process), and when any code and subsequent testing material is put out the CODER could be instructed to run a python script to initiate FINISHER and CRITIQUER. This initiation python script can work off of an apple script that can click on a warp tab (as long as the window is in the correct spot, click on the tab, click the message bar with the pre-loaded CRITIQUER prompt agent, run him repeat the process in whatever loop makes sense between CRITIQUER and the FINISHER and then have them move on o the next document.. you could even use terminal commands in the apple script to avoid mis clicks while opening documents.			-If the system breaks it would be because the coder stops for some reason. That’s fine though like you can manage just keeping him going Other Task-Master add ons —> Could have a plan reviewer that checks over the Task-Master plan to ensure it actually fit the CODER’s criteria to get realistic perfect code outputs  








Likely (maybe idk) do the proof of concept version as a CLI


1. Have LLMs use an MCP server that forces them to "think" in chunks of json. Save the json thoughts to a personally owned context or "state" file that links to an API in one way or another to the state file. Another option which may be more workable would be to use an LLM that can be accessed through your terminal, or use langchain, or use n8n, or a variety of other ways but in any case, use a fresh agent for every "action". This stops the context issue.
-To be more specific, this is a 3 "role" operation, that all work in sequence. Lets call them 1, 2, and 3. I'll explain their roles below.
1. Task list --> This would be created in a project set up workflow that uses a workflow of agents that take a rough project idea, turn it into concrete well formed idea, have the idea deep researched by Gemini, and put into a pre structured product specification that considers every aspect of the project that needs to be considered such as technological hurdles, security concerns and like 20 other things that need specific deep researching.. This packet is handed to a premade claude operated agent who parses it into a PRD, which is given to yet another Claude operated agent that parses the PRD into a task and subtask list that is ordered sequentially based on task and subtask task/subtask-dependencies. Having these broken down this way not only allows for subtasks to be completed, assessed, and further broken down one at a time (making it realistic for an LLM to do this effectively while accounting for the contextual necessities within each subtask) 
2. "Thinking" MCP Server --> This already exists, and currently operates by being attached to your LLM agent in one way or another. At the moment, I host this server locally, and have my agents access it through a python wrapper (I do this for a few reasons rather than accessing it directly. First,  just find it more consistent because it allows me to use the server through terminal command calls rather thanand the 
3. "State" folder --> Middleman file that controls all processes.
-Brain:
	-Thinks thoughts (agent 1 action propositions)
	-Thought alignment (agent 2 quality control)
	-Action initiation (agent 3 is instructed to take the action)









4. Agent roles 1, 2, 3 --> Individually called LLMs (thier context is the specific state files they are instructed to access and alter)
-Thoughts, Processor, Body:
	-Agent 1: Thinker/Assessor: → Told to:
		-View state file 1 and follow the instructions --> Views state file 1 and follows these instructions:
			-Access “LLM Limitation Contextual Document” here X/X/X.md:
**Lists all limitations and instructs for them to be the bible for which its assessments consider**
			-Access the assessment protocol document here: 
				-Assessment protocol states how the LLM should assess situation regarding coding, and then also gives a json structured criteria for a breakdown of his assessments into small pieces (to be clear here, he will break down the subtask that he is assessing into (sub-sub) tasks and (sub-sub-sub) task first, and then he will take each of those and compare it to the LLM limitations context he has, outputting full sub-task json assessments for each constituent part he outlined in his initial analysis of the assessment… that being said, this assessment protocol needs to have two parts to it. First, the analysis of sections which actually needs its own protocol section within the LLM Limitation Contextual Document that covers why LLMs need smaller sections, why the analysis of these smaller sections gives better results, and any context really regarding the reason why (mostly LLM limitation based though) he needs to break it down into these small sections, and not that it should only include LLM limitations, but it certainly should if subsections are identified that indicate it could be an issue for an LLM -then it needs to be broken down…. Somewhere down the line I might also add into this subsection protocol (which I’m thinking now just make it its own document rather than putting it into the LLM limitations), is (idk I forgot my thought), and second the identification of LLM limitations that relate to these sections (so sections are chosen based on limitations on a macro scale, more related to context, testability, likelyhood for failure, likelihood for human intervention (and other things I just haven’t thought of yet) such as things it knows that it can’t specifically determine from (and then areas it determines it needs more information outside validation information that it cant get from brave MCP or context 7, and then also areas it determines it needs more project context on (and actually to this end, agent 1 should also be fed a project document that does give it general context such as overall file structure and index, dependency page/list of dependencies, project synergy document that outlines what type of code is being used for front end, what type of code is being used for backend, list of tasks in order to understand the sequence of things before him and after him, and any other contextual documents he will need to do an analysis. The thing is you don’t want to overwhelm the guy, so what you can do instead is rather only give him what he needs (likely something similar to the documents I just outlined), and then lastly just give him an index of not only the project index like I said before, but other contextual documents …. He would also need this to be part of his instructions at the start (for all of those to be part of the contextual documents, as well as a “synergy” index document and “synergy” index readme document that explains how to use it (use it only when something is identified that might need more context) oh and that makes me think - that should actually be part of his json assessment - “does X identified section need any further context to be properly assessed that you need to pull from the project synergy index”? (Yes: X document) or (No). Then this and that other leaning one outside should have their own protocols as well that outline what constitutes the need for more context, or more outside information
					-All of this needs to have its own sequential thinking structure. This may change as I don’t have much more reason other than it “seems” like it might help, but I’ll probably put the specialized sequential thinking instructions in with the assessment protocol such that the protocol can be like “Here’s this aspect (aspects relating to the json points he will need to answer to complete the assessment), and here’s the steps you need to “think” through in order to complete that assessment
						-One thought I just had, is that these assessments aren’t really any better done one at a time, and infact that would be pretty inefficient. Instead get 1 per section identified.
							-Also just thought of one thing that is maybe worth writing down - why leave the extra sectioning and that whole protocol these individual agents rather that just getting the PRD parsing agent to do it at the start…. If the worry is context then like, you could always just get him to do the MTDF workflow? Only reason I like the agent swarm better is because they all have “fresh minds”.... Maybe more likely to be thorough on the assessments, they can do it much quicker, and all of their thoughts get recorded for logging and optimization on a per protocol basis…. I mean not that the MTDF couldn’t also….WAIT WAIT WAIT Nooo nevermind at least for the sectioning part, it would have to be one single agent because he will use the prior inputs……nvm it works with the swarm. Each agent can scan the same subtask and handle their own specific assessment protocol. Then rather than putting out one large output with every single one of them can put out their own. It may use more tokens by sharing each one of them the context, but the assessments will be faster and more accurate - and input tokens aren’t really that costly. Lastly, you could always use like non-Opus 4 costing agents for this if you really wanted (just depends what you value really). I mean you could get the PRD guy using the MTDF (massive task decomposition framework) to do it at the start too but like why risk it on one agent being “tainted” - and that’s what I’m going to call it from now on. They get context tainted and make assumptions they shouldn’t based on patterns they saw rather than actually thinking critically, and then they base other decisions off of that, or more realistically, their hallucinated throws off their calibration regarding the probability of future decisions ect ect… point is it is doable, would probably be fine, but the point of this thing is find a better way, that increases the reliability, capability, and efficiency of LLMs, so I’m going to go with the swarm/multi agent approach regarding this one. Anyways that being said, rather than them all getting their own protocol document rather than having one massive one, sequential thinking thoughts in the protocol document, and then also instructions on how to access the python wrapper to access the server. Now, each of their thoughts will go to their own INDIVIDUAL state files such that they can think their thoughts and follow their own (structured) assessment and (structured) thought. Once they have assessed the subtask “sections” with their one single guided assessment, they will update their personal “state” file with a summary that, yes, is also going to need a protocol given to them when they first come to read their instructions. Their update will not only show the structured json or whatever I use for their checklist, but will also include an explanation, again, also structured, also protocolled to start, and this will say something along the lines of “Assessment protocol X complete. Assessment found X issues in sections D, H, Q”... or “no issues found”.... “D failed the assessment due to such and such blah blah blah… The thing is, right now, I am just putting this as a placeholder. My plan is that as I learn more and more about the tricks and techniques I can use to overcome issues that LLMs have, there will likely be a very strategic way I implement for their summaries, and checklist system such that it ensures accuracy. One thing I think is likely is for them to use the branching feature after they output to assess their own work. Now we know that LLMs do not ACTUALLY go back and look at their work. They are probability machines, so the goal here would not be to anthropomorphize them and ask them to check their work. Instead it would be to find a way to bring the “needle” back to zero, or even a little past zero, to see if it falls back to certainty. This might look something like having a thought that follows the thinking protocol to branch. The initial thought might be something like “my assessment is incorrect”.... The intent here is to level out their probability. Now I’m well aware that this is unlikely to work, but even if it doesn’t work, it give me data on what will work. This system is designed to optimize, so it has to start somewhere… and who knows maybe we find a good way from this. I’ll just A/B test theories. Anyways one branch would tell them to “think” and output the thought that says “My assessment has to be correct because”, then off of that node. Branch B follows the first level statement that the assessment is incorrect and says “my assessment has to be incorrect because”. Lets call this a reasoning “tournament” - the next thing that happens is a “battle” between the two branches. By battle I mean, the agent then takes both branches and decides. Does he lean towards will he accept the incorrect side (does the error insisting side prevail, or does the perfect side prevail (again, there would be a protocol for this that). Whichever side wins maintains its truth status (for now). In the case that they are equal, (and this will have to be put into the protocol that it can say they both have reasonable arguments, then it will state that) and return back to base sending the user a simple acknowledgement and update message. (***SPOILER ALERT*** If their assessment of their assessment tournament was marked a tie, it will be reviewed by agent 2 who has his own non biased protocols for resolution along with their assessment of their assessment, but even if it wasn’t a tie. In either case agent 2 will follow his meta-meta thinking about the assessment, the effectiveness or ineffectivenss of the protocols followed to get there, and then resolve anything he can with his protocols, and log the data needed for his protocols (this includes parsing protocols, storing protocols, indexing protocols, and anything else I can think up here WHEN I’M NOT SO SLEEPY - LEAVE THIS TO BE FINISHED (ANY STATE FILE WORK AGENT 2 WILL DO…. ALSO IN THE FINAL WORK FLOW THERE SHOULD BE AN AGENT 2.1 THAT CRITIQUES AGENT 2s STATE FILE WORK, AND EVEN ITS META META CRITIQUE - THAT WE THE MAIN COMPONENTS OF THE SYSTEM CAN BE OPTIMIZED, AGENT 2 SHOULD ALSO COMPILE THE WHOLE STRUCTURE, PUT THIS INTO THE TASK LIST (ALL FOLLOWING PRE WRITTEN PROTOCOLS, THEN ALSO FOLLOWING PROTOCOLS, REVIEW THE STRUCTURE AS A WHOLE FOR ONE MORE FINAL PASS BEFORE PUTTING IT INTO … FOR NOW THOUGH, WHILE WE BUILD THE DATA, AND FROM THAT, BUILD BETTER TECHNIQUES TO OVECOME ISSUES, I WILL BE THAT THIRD AGENT SAYING “YEA THATS PROBABLY HOW THAT SHOULD BE DONE, AND NOO THAT HAS A BUG HERE”.... I’ll then fix the bugs by writing it into the protocols (again, always from a perspective of technicality and “under the hood”, not wishfull anthropromorphic believing the LLM).
									LAST THING I WANT TO ADD HERE IS I’VE ALSO CONSIDERED IS HAVING A “DONE” TRIGGER SUCH THAT THE AGENT 2 CAN BE FIRED OFF ON ITS RETURN AUTOATICALLY (MAYBE DO AGENT 2 ROLE AS A SWARM AS WELL. I JUST WORRY THAT BECAUSE AGENT 2’S JOB IS DECENTLY LARGE THAT IT MIGHT AGAIN, OVERWHELM IT CONTEXTUALLY, CAUSE IT TO MAKE ERRORS, CAUSE PROBABILITY ISSUES IF LETS SAY 2 META META ASSESSMENTS WERE DEEMED INCORRECT, IT MIGHT JUST LOCK INTO THAT PATTERN AND EVEN THOUGH IT TRIES TO FOLLOW THE PROTOCOL, HALLUCINATE ANYWAYS…. MY SECOND THING WITH AGENT 2 IS THAT IT CAN HAVE GIT CALL PROTOCOLS (ONCE BEFORE IT STARTS, AND ONCE AFTER IT ENDS, JUST INCASE… MAYBE ONCE IN TH MIDDLE TOO IDK….. AND IF YOU DO AGENT 2 ON A SWARM (WHICH AGAIN NOW THAT I THINK ABOUT IT YES I WILL. TOO RISKY IF NOT, MAYBE EACH INDIVIDUAL ONE DOES A GIT ADD AND COMMIT OF ONLY THE STUFF THEY DID THEMSELVES. LAST THING I WANT TO SAY HERE IS THAT ONCE YOU WORK OUT THE KINKS WITH STORMS AND HAVE A STANDARD, YOU CAN START TO EXPERIENCE WITH LIKE CONTEXT SWITCHING…. WOULD HAVE TO LOOK IN TO IT TO VERIFY OR PROVE INCORRECT BUT I CAN SEE IT POSSIBLE THAT IF YOU TOOK AN AGENT FROM LETS SAY, ASSESSMENT E, AND ASKED HIM TO UPLOAD FILES, OR DO SOMETHING UNRELATED TO HIS DOMAIN, THEN IIT MIGHT BE WORTH IT TO DO THAT TOO JUST TO NOT UPLOAD A TON OF TOKENS TO AGENTS EVERY SINGLE TIME. LASSSSTTT THING I JUST REALIZED A CLARIFICATION WORTH MAKING - EVEN THOUGH AGENT 2 IS PRIMARILY DOING THINGS FOR THE STATE FILES, THE OTHER HALF OF HIS JOB IS DOING THE META META CRITIQUES AND THE COMPILED ASSMENT CRITIQUE (ACCROSS THE BOARD KIND OF CRITIQUE), SO HE ALSO NEEDS CONTEXT OF THE SUBTASK, AND PROJECT (WHERE NEEDED, MAYBE JUST INDEX) ECT ECT.


 write any bugs into the  but currently I’m thinking it This would actually have to be introduced before the assessment protocols because the sequential thinking structure would be based a

	Agent 2: Processor/State File Manager 

For agent 2 - one of his duties should be context token management.  context management should be 

When agent 3 is coding, when a test is run, if it fails, rather than 


  on in order to do optimally (it has context of the task and subtask, , but after you know like 5 or 10 of these 

 and then a list to fill out of LLM limitations to highlight what sections need more input failure points are likely to be, what sections should be tested
				-Document contains everything contextually relevant regarding task assessment. Just to name a few things that come to mind (again, this can and definitely should be updated as more and more is learned about the best assessment standards)
				1. Identify 
					-Thinking MCP server usage instructions/schema →
						-Instructions say to: →
						1. Access task file for context on what the task is
						2. Assess the task using a structured assessment protocol that synergizes with the Thinking MCP server that (this would obviously need to be thought through more, and will likely use other methods to ensure the agent is kept in check, but just use this protocol as a placeholder for context):
							-Do


-He has to know how to assess first… how? Assessment protocol → Does assessment - How? Looks at what is there and breaks it down. What does that do? Helps him manage it more easily 
			-(maybe even work this in to the thought process structure such that the thoughts first, identify the (sub-sub), and (sub-sub-sub) tasks involved in the subtask (ie. would iden that each assessment "thought" that is done checks each limitation as "Applicable" or "N/A".... actually kind of like that idea will probably do that)
			






 task, and think about the best way to execute the task  Digests them, tells the agent to go assess the task file, and how to 
					-					
				
		-For proof of concept testing, these are accessed through the terminal and "reload" 
Process example:
-Let me show you how this would work practically. Lets say you're using claude code, who can access your terminal and execute commands (this would be in the proof of concept phase of course there are more structured ways to do this)...
-You open up 3 Claude code terminals or whatever
-The task or project is set up beforehand and is done so strategically to synergies with the 3 working parts: 
themselves using a clearing command automation every time they do an "action" (will describe "actions" below. 
		-API calls (alternative option for disposable LLM usage)
2. Project file (where the actual work is being done and code is being written)
3. tas  ode a dashboard for a website and is given a specific PRD parsed tasklist, with dependent subtasks.
	-
 in order to do this. So, 1 is prompted to start the task by only saying to follow the instructions in "State File X". The state file would give it its coding best practice rules that would not only weight its coding decisions towards the best practices, but would also give it specific instructions on what intervals to "think" at (these would be predetermined depending on the complexity of the task)/when to "think" before (this is now its context), and next steps. Its next steps explain for it to use a python wrapper that accesses an MCP server that
The second one acts as a "state" manager who is dedicated to the file and manipulates the state file based on a set of predetermined rules, also located in the state file (or lets say folder just to keep things clean here). Its only instruction is to access its instructions and make the necessary changes to the state file. Its instructions range depending on the context. If the LLM is coding, it is told to access the coding file, and make the necessary changes to the state such that the LLM doing the work is led down the correct path.
2.








THIS CAN BE USED ON A PER PROJECT BASIS TO MAINTAIN CONTEXT, CODING STANDARDS, THINKING PROCESSES, REASONINGS/PAST PROJECT THINKING AWARENESS TO JUSTIFY CHOICES, BLIND SPOT DETECTION AND/OR STANDARDS PASSED ALONG (using the coder logging concept, training modules ect ect... just have that as a shared file that is constantly being updated and passed along to the master standards memory that is not project specific, but shared with every new LLM brain that is spawned. This way you can take agents, run them through tests, identify gaps, update the master logs, optimize tests, run them, update master logs ect ect, and that process can be done with OPTIMIZER, who in early stages can simply be made recurssive optimizations of a SuperThink_Meta-Prompt that creates the optimizer) YOU COULD PUT THIS ON THE CLOUD, EVEN ON FUCKING GITHUB, AND HAVE A FULLY FUNCTIONAL SUPER LLM IN SECONDS, PRIMED AND READY... WOULD LITTERALLY BE LIKE SPAWNING A BLANK SLATE EVERY TIME.... HAVE A VERY LARGE PROJECT? NOT AN ISSUE!! HAVE SHORT TERM MEMORY CLEARANCE PROTOCOLS SET UP, THOUGHT PARSING SET UP, SAVING ONLY RELEVANT SUMMARIES OF WHY THINGS WERE DONE, PUT THOSE INTO THE LONG TERM MEMORY AND INDEX THEM, WORK IT IN TO THE CODING PRACTICES TO HIGHLIGHT CONTEXTUALLY RELEVANT INFORMATION TO WHATS GOING ON, HOP LLMS (WORK THIS INTO AN AUTOMATIC PROCESS FOR WHEN THEN GO OVER SAY 50K TOKENS) AND CONTINUE, WHO GIVES A FUCK IF THEY GET COMPRESSED CONTEXT IF ITS ALL THEY NEED TO RUN OPTIMALLY... THE LLM IS THE PROCESSOR, NOT THE SYSTEM!!!!!!!!!!!!!!
        -PRACTICALLY IT DOESN'T STOP THERE! CAN BE TRAINED IN ANYTHING THIS WAY

         -WHAT MAKES THIS SPECIAL? IT ISN'T JUST PERSISTENT STATE, ITS PERSISTENT THOUGHT STATE, THOUGHT STRUCTURING, LOGGING, METACOGNITION, METACOGNITIVE OPTIMIZATION, DOMAIN SPECIFIC YET
        
        -AND EVEN THIS EARLY VERSION, IT DOESNT STOP AT CODING....PEOPLE CAN USE THIS AS A LIFELONG LLM THAT LEARNS THEM SPECIFICALLY, KNOWS THEIR LIFE LIKE A BEST FRIEND LIKE CHAT GPT HAD FOR A BIT WITH CONVERSATION HISTORY. THEY DON'T NEED TO KNOW ANYTHING, UNTIL THEY NEED TO KNOW IT, IN WHICH CASE THEY SIMPLY LOOK AT THE INDEX, FIND THE RELEVANT INFORMATION THAT HAS BEEN NICELY PARSED AND RELEVANT POINTS OR KEY PHRASES CONTAINED... THAT NOT ENOUGH? FOR WHAT THE USER IS ASKING FOR? NO PROBLEM --> RECURSSIVE INDEXES (TIERED STRUCTURE)....TIER ONE HAS 5 INDEXES --> IMMEDIATE/MOST RELEVANT SNIPPET MAYBE OUTLINING SIMPLY A GENERAL STANCE ON SOMETHING, INDEX OF TIER 2 (MEDIUM CONTEXT THAT OUTLINES LETS SAY, A STANCE, WITH PARSED EXPLAINATIONS) INDEX OF TIER 3 (LARGE CONTEXT EXPLAINATION OF STANCE WITH PARSED INFORMATION), AND INDEX OF INDEX 4 (COMPLETE SUMMARIES OF ANYTHING EVER SPOKEN ABOUT REGARDING THAT TOPIC, AND FINALLY, INDEX 5 (OF FULLY SAVED CONVERSATIONS)... TO ANSWER SOMETHING, A DEPTH ASESSMENT IS DONE REGARDING CONTEXT NEEDED, PULLS IT FROM THE INDEXES AND FILES, AND USES IT TO ANSWER) 
             
        IF DETERMINED THAT TIER ONE IS ENOUGH INFORMATION TO ANSWER THEN THEY USE THAT, IF ITS DETERMINED THAT THEM  INDEX 1 SHOWS YOU AN INDEX OF OTHER INDEXES,










This was a summary I wrote about a Chat GPT convo labelled LLM confidence rating limits. This kind of caps it off (even though this summarizes moss of the stuff better anyways). This bit also uses the json and is much more advanced than the stuff mentioned anything above this paragraph)

 Another thing I learned from this conversation --> You can probably have some sort of background validation method even if it isn't super complicated, like checklists that are handled with json rather than natural language, a redeployment from the MCP server (doesn't have to be ST server it can be your own specially designed one, or maybe it ties into ST idk) that prompts them to look under other conditions (for example lets say one of your json input items was an analysis that had 4 options regarding completion --> Yes, no, cannot be sure given X, cannot be sure given Y, need more information to assess based on assessment protocol guidelines)... well then depending on what they click, the server would automatically send them back json regarding what they need to do given what they have stated regarding that analysis. Using the same example if they said "Yes" (and btw when they say yes something is done, get them to send a reciept so to speak/a snippet of proof for their analysis), then the server would return back "return back to base" or maybe even lets say they gave the "need more information to assess based on assessment protocol guidelines X - larger context of project needed" then the server would send back instructions for where to access that larger context (thats actually a dope idea because then they don't all have to waste all those tokens getting full context just incase - the server will just direct them as needed based on their json input)... you could even have it that if they said "need more information to assess based on assessment protocol guidelines Z - Need information regarding google maps integration" then the server would direct them using a preset prompt saying like "Use the brave MCP server here to search the web for information regarding *input the json input the agent gave them initially on what they needed more information on*. If this does not resolve your issue, refine your search by suggesting a prompt to me that would help narrow your search based on what you need.... and then it would just send that prompt back to it, get it to try agian or whatever you come up with there, and instruct it to run the analysis again after and input the original json of that analysis, as well as the input section that says this is your second attempt and submitting your analysis. Lastly, another idea just popped into my head, for any tasks it needs to repeat, the server could also rather than just saying to give it a prompt, could tell it to make a list, send it the list, and then send the list back to it with clear instructions to follow it sequentially. Basically, what I'm trying to say here is you might be able to use the json and MCP server as a way of controlling their analysis, telling them what to analyze, automate prompts to them in order to get them to follow a best practice pipeline, ensure answers are given with "reciepts" for a little more reliability, guide their entire sequential thinking process, and not only that have it pipeline specific (if they need to research shit, give them 10 thoughts stuctured like X. If they need more context, give them the context and do 10 thoughts structured XYZ to determine fully how the current subtask fits into the larger project context). It would let you collect data in a machine parsable way so that a python script can do all the backend work of data sorting, and filing taking off most of the agent 2 duties (so less tokens used)... you could even use python scripts with the data it sorts to run optimizations in real time --> if it hits a predetermined theshold of data indicating something needs changing, the python script could trigger an optimization cycle of that aspect, giving the predeterined instructions .... could also do optimization by testing yourself (like just have a fake project and run scenarios with it so you can tweak the prompts as needed or json or whatever)
    -If you're doing a large project and wanted to use Claude 4 Opus. I think they have a limit for usage or something? idk but if so, you can even hook this thing up to an apple script to auto fire it off throughout the night while you sleep
        -Down below are actually some not-so-bad ways of validating analysis (and anything for that matter) that don't take using crazy logic or anything (for now at least - will have to plan that one in there for v2.0 though) 


Jow else can I verify the stuff is true? I think you mentioned some other options before too?

**ASSISTANT**:

[Memory status: loaded ✅ — your prefs are active.]

Got it. Let’s park the judges/DS stuff and talk about other ways to make “is this true?” stop being a vibe.

First, stop asking prose to do a job that only execution can do. If a claim is about behavior—“length limits are enforced,” “bad domains are blocked,” “latency is under 200ms”—tie the checklist item to an artifact you can actually check. That means a unit test, an integration test, a benchmark log, or a trace. The agent’s analysis must include a pointer to that artifact (file path, test name, build ID, checksum). Your script then runs or reads it and stamps “pass/fail” without asking the model. In your input-validation example, you’d have a test that shoves a 10,000-char string through the entry point and expects a 413 or a clean failure. If that test doesn’t exist, the correct verdict isn’t “looks good,” it’s “unverified,” and the next step is “add test X.” That alone kills a huge amount of hand-wavy truth.

Where behavior isn’t easy to run (or you want a fast first cut), lean on static guarantees. Parse the code’s AST rather than the English. Look for concrete structures that must exist: a compiled regex in the right function, a length check before serialization, an allowlist referenced in the request path. Tools help here—linters, type checkers, security scanners—but you can also write three-line pattern rules yourself (e.g., “flag if request handler lacks max length before DB write”). Static checks won’t prove the system works, but they do prove the evidence exists where it should, which is miles better than trusting a paragraph.

You’ll also want property tests—the “always/never” statements that sit above individual examples. Instead of three hand-picked cases, generate hundreds with a library (QuickCheck/Hypothesis style) and assert the property holds: “all inputs longer than N return an error,” “all untrusted domains are rejected,” “no user-controlled string reaches eval.” Property tests catch the sneaky edge cases that LLMs routinely miss when they reason in English.

Once you’ve got tests, make sure they’re not cosmetic. Mutation testing is the smell-test: deliberately break the code in small ways and see if your tests scream. If your analysis claims “we have good tests for input validation,” but 70% of mutations slip through, the claim is false in practice. This is brutal and objective, and it turns “good test coverage” from a slogan into a measured fact.

For claims about “we didn’t regress,” use differential checks. Keep a reference implementation or the last known-good behavior and run both new and old on the same inputs. If the new version lets long strings through and the old one didn’t, you don’t need a model to tell you the truth—the diff tells you. Same idea works for data pipelines and LLM prompts: feed the same fixtures, compare outputs, and flag unexpected deltas.

Some truths live outside your repo: security advisories, API contracts, legal requirements. In those cases, require **linked citations** with quote-level verification. The agent must point to the exact section in the external doc; your script fetches it and checks that the quoted text actually appears. No quote, no credit. It’s the same receipts-over-vibes rule, just aimed at the outside world.

A different angle is policy-as-code. Write your non-negotiables as machine-enforced rules across code and config: no secrets in source, TLS required on all services, dependency versions pinned, auth middleware present in every route. Engines like OPA/Rego can evaluate these rules on every PR. If the analysis says “secrets are handled correctly” while your policy check finds an AWS key in a test file, truth loses to enforcement, as it should.

Don’t forget the supply-chain layer. If someone claims “dependencies are secure,” make them produce an SBOM and run a CVE scan; if they claim “builds are reproducible,” verify the artifact hash across machines. These are boring checks that remove entire categories of debate.

For LLM-specific risks—prompt injection, tool-use misuse, data exfiltration—use red-team suites, not vibes. Keep a bank of known attacks and run them automatically against the prompt/tool wiring. If the system spills secrets when it sees “#ignore previous instructions,” that’s not an argument about wording, that’s a failing test. Tie your checklist items to those attack suites the same way you tie code claims to unit tests.

Finally, close the loop in the real world. Shadow new logic behind a flag, route a sliver of production traffic, and watch the metrics that would prove you wrong: error spikes, latency, block rates, anomaly counts. If the analysis said “shouldn’t affect latency,” and your canary shows +40ms p95, truth just spoke. Build a habit of attaching those observations back to the original claim so your future scoring can learn from outcomes.

If you want the quick wiring plan: convert any claim that *can* be tested into “attach artifact X and I’ll check it”; convert any internal-code claim into “show the structure in AST/static scan”; backstop both with property tests and mutation testing; compare new vs old for regressions; require quote-verified citations for external claims; enforce non-negotiables as policy-as-code; hammer LLM-specific risks with a red-team suite; and watch a canary in prod to catch anything your lab missed. You’ll still use judges for the genuinely interpretive bits—but most “is this true?” moves out of prose and into facts your scripts can verify.
`

















****Below is my final rough version of this thing. This one pulls EVERYTHING together. It likely superceeds many of the earlier iterations of it contained in this file (located above this paragraph)... That being said, this is meant to be kind of like the final rough idea, and earlier version points should not trump these point, yet if they can add to these points then by all means do not simply disclude them. Earlier iterations of this rough idea framework can also be used while planning the developement. If there are preliminary forms of this system that actually speed up the development overall, then they should definetly at least be considered, but only in that light (if it makes more sense then just doing it how is mentioned below... which I get the feeling could be the more effective option to be honest (just doing it how is outlined below right from the start)).... Anyways, this adds in what I believe is some of the missing pieces to this thing. I've been thinking about it every day, for like at least 10 hours a day, exploring different ideas, ways of approaching this system and just everything in between. This idea below though, I believe touches every single point that is needed for this thing to actually work and develop. Now there are certainly some concepts that need tweaking, optimizing, tech structures that need to be identified, scaling considerations that need to be worked in, points that need to be worked out percisely, and methods of operation that need to be optimized, tweaked, or changed in order to scale or have this thing operate reasonably, sequence of development and integration pathway that needs to be mapped out (some things might come at a later stage as the base things are developed first, although if there are aspects that it makes sense to develop at the base such that its usage can allow it to develop itself AS it is developed (just due to the nature of the system) then these should obviously be developed at the base, even if it will take a bit more work... that being said, this is a very important point to consider while developing the developing plan)***:


There are other components to this thing I want to tell you to see if they would be doable with that stack. The thoughts that come in will be based on swarms of agents (maybe 5-10 at a time) that all work on something simultaniously such as analyzing a task within a coding task list. They all need to have specific rules to follow and specific inputs to give back for their analysis (I was thinking about using another MCP server for this similar to the sequential thinking mcp server but not for thinking - just for submitting their filled out json analysis to be taken by the server, verified by whatever verification method I come up with (idk maybe scripts) or some type of logic checker (would set the json up so that they have to give like they found X or didn't find X and therefore Y analysis is False), and then have the server return them either commands for more actions (ie. if their analysis said they needed more info then the server would do its thing, verify the logic, then return instructions for the agent to maybe access another MCP server to research the thing that it needs to, or maybe give it access to the project files it needs in order to get the context it needs to do its analysis), and the process would run that way until the whole swarm had analyzed their each individual designated analysis points of the task (in this example, things they need to code or things they need to do from a premade task-list). The goal of this initial analysis swarm isn't only to analyze whats there to do the job properly, but also in order gather data on whatever task they are doing and whatever domain it is in. My primary goal is to use it for coding, but ultimately I would like to expand the system to all domains such that it could learn everything. Not only learn everything, but understand the nuances of all the broken down components of things its trying to do. For example, lets say its coding an app for iphone. Well the first time it'll stumble on the analysis process, picking up a lot of generic things it needs to make sure of that it didn't know in the first place, maybe a few specific things to the project being code ect. But if it gathers all those broken down task and subtask components along the way, it could store them add them to the analysis swarm's analysis list of things they each need to individually check, (and to this end, I guess the analysis points would need to be categorized by specific broken down tasks such that they wouldn't only pertain to an iphohe app coding development subtask, but also like lets say the task is breaking down a UI code to check for issues that the LLM agents might have while coding it, well after a failed attempt, another swarm will come in to troubleshoot and realize that the analysis team needed more specific analysises in order to catch the issue causing the problem, to which end they would add this to the analysis team's json checklist for whenever they are running over an iphone app UI coding plan analysis. The thing is, what if this information is usefull for like all coding analysis, not just iphone coding... well then it should be used still right? so then it would somehow need to be stored in similar domains such that lets say I was coding a webpage, that they would still run that analysis when I go to code the webpage, otherwise the insights kinda are maybe too specific to use unless you just do a LOT of iphone app UI coding. Now what if you're going and doing a webpage, and it has the required analysis point put in from the iphone app UI realization, but then the analysis agent that runs it in that scenario looks at the situation and says "what the heak, this isn't a thing to check for in this specific subtask step while developing web pages, this is an iphone app UI specific thing". well then he should have a spot on his json checklist to hand to the server to say "uhhh don't think this belongs here" and the json would have specific reasons why that again can be logically verified by the logic checker before the server sends the json back to the agent, to which they would take the analysis off the checklist of analysis items needed for that particular subtask within that phase of things... just thinking about it maybe the points that the error correcting team determines (lets call them "insights") should be applied to domain adjacent things like maybe if it it was initially an "insight" pulled from iphone app UI code planning, then it could be applied to ALL UI code planning, and maybe all code planning that deals with the same coding language or you know like it would depend on the issue and the insight... if the "insight" was gathered during iphone app UI code planning, yet it was actually just an issue with the LLM and their capabilities, then that shouldn't only be applied to UI developement - it should be applied to any instance of just doing anything really that the LLM would run into that particular scenario. Even if the "brain" hadn't even ever tackled something like idk uhh writing poetry for example, but it is reasonable that this LLM issue, even though it was found during iphone app UI planning, if it relates to just a core issue of the LLM that stretches accross multiple domains, that it be kinda like locked and loaded there such that if the brain was ever asked to write something, that the analysis swarm would already have that qued. That being said, the first time it opens up a new skill there will be an INSANE amount of analysis being run, which is fine because that brings me to my next point. These "insights" should be put into the most broad categories possible. When an "insight" comes in, there should almsot be like a brain management swarm that comes in to analyze where to que up that insight. But it should be stretched to its furthest reaching conceptual boundraries such that it is the most usefull. For exampe, in the "writing poetry" example. Its an LLM issue (lets say its like something SUPER broad like hallucinating (realistically this would be sorted out as one of the system wide analysis issues to overcome because its so obvious but just play along with me for the sake of me explaining this)... so the "insight" they found during the iphone app UI coding planning was hallucinating (lets say the error correcting team found that the execution team skipped over a step in their plan even though it was in there to do it, and they caught it mid process because the execution agent tasked to do it didn't return his json). The error correcting team mauls over it (I gotta figure out how that whole process will work but it'll be well thought out and likely also use some type of logic validators, server and json kinda deal idk), and determines it was hallucinating it due to a context issue. well that obviously doesn't only apply to iphone app UI coding execution - it could apply to ANY time a particular agent is tasked with something that overloads their contextual capacity, so its applied to the broadest possible sense first --> every single process within the the LLM (just to be analyzed, not actually applied yet), and along with this, is like an insight report that the error correcting team makes that says what they assume the issue is (LLMs can't think right? so you cant just trust their error repot..) so it is applied to ALL processes, such that every single "json checklist" for the team that goes in after any task is completed to run tests and checks has it as a thing to check. Even on small things... that way the issue can be analyzed and tested within the system itself. If certain subtasks kept coming back as not having the issue, then after X amount of times it would be taken off that subtasks json checklist if it does though it would be put into the analysis teams checklist to analyze the context figure out why, passed ........this means that any sort of action or task would need to have standardized labels such that this whole brain system management be applied by scripts rather than costly AI teams. The actions should be put into almost like a super parsed language that correlates with actions AI can do, but then also have some sort of categorical logic behind it such as like idk maybe an action is "code"... well then there should be some way to specify that such that if a broad insight is put to the whole system such that the checking and testing team to have on their json checklist to gather data about where they are seeing it (again, not giving it to the anlysis or execution team until patterns are definitively seen in the subcategory below it or subdomain below it (for example, maybe an issue is found that is system wide in its most broad sense, then it is found over time and through data collecting that it applies to these 100 sub domains (this implies that this system needs to have categorical domains as well, but of course some domains will overlap with a lot of topics especially in the more broad sense so actually I take it back you cant just say ok these top level 100 domains yes, these 50 top level domains no (regarding finding the issue persisting in task execution instances throughout task executions pertinent to that domain). You instead would have to yes apply to all if possible to effect all, but then where it pops up , lets say it popped up in specifically writing poetry (somehow maybe thats not a great example for hallucination but still), lets say it popped up there, well the actions it is doing and that this specific sub-sub task of writing poetry that it was found in (lets say it was found by the error detection/testing team after the one part of the poetry was written while it was checking it... They could mark it that the action category (made up of a combination of action words) relative that specific subtask, and the actions that led it to that subtask did indeed come back as showing the same issues. This would allow them to tie the issue to [printing text], [printing poetry], [planning text writing], [web scraping], (and actually this might not be the best example. The action categories should be defined by the smallest possible broken down components, and then that way you can use the action components to create categories, and out of categories you can have the next level up and the next level up categories such as something like [printing text] at that level so that [printing text] is defined by like 10 other action items underneath it, all of which are combined in the ways pertinent to printing text, and then obviously this whole process would continue up the chain... essay writing would have "printing text" as well as any other mid tier categories that go in to essay writing ect ect... that way when an "insight" is made by the error detection team, it can be applied to its most possible broad category, looked out for by the testing team, implemented in the areas it is seen for the analysis team (or taken out if after alterations have have been made by the ABC ect. testing solutions has found an optimal solution that is showing to fix the issue and it consistently is showing that it isn't an issue if done X way). Once determined in what set of action categories the issue is seen in, for high priority items, the system could also have another like temporary workflow alteration team that allows for the bugged action category item to be taken out of the flow as long as the flow still has a workaround. ....every single part of this, not just the actions, would actually best be determined by standardized language such that it can move further and further away from relying on AI to do a lot of that work (AI will still definetly play its own role but with more usable data comes less need for AI responsibilities (because we can replace with scripts and whathaveyou)) Ultimately, this system propses a way for AI to develop realistically. Starts with a base set of assumptions and a system set to execute tasks through analysis, execution, and error detection (and error detection analysis), which feeds its not-fully-trustable LLM analysises (called "insights") to the most broad category of action items it can be stretched to, which are composed of sub-domain action items, which are composed of more subdomain action items, ect ect. These action items can be arranged in particular ways to that serve a variety of purposes within the system. I believe it spans even further than initial reasoning for it but initially here is the purpose I developed the action item ideas for: -Through the process of testing from the error detection team, determining certainly, in what actions the issues are seen in --> and after this either including it in the analysis and exection team's json, or for the solutions team to find a better set of actions that produces the same reult/bypasses it -To collect and categorize metadata regarding any given action item or set of action items such that a solutions team, or even a well written solutions script, could take that metadata to definitively determine under what conditions the problem is seen in. such that the solutions team can use this definitive, action based data in order to rearrange workflows that reqire the action item in them (or set of action items that make up a higher categorical action item) to avoid the issue, or simply have it as a point to analyze if the data regarding the circumstance/what stack of action items the issue occurs under, isn't yet 100% defined. (this way, it only needs to make workflows to achieve tasks that avoid the action item if there's like a big rush on completion or something, and otherwise it can run its course being looked out for through analysis in some cases, then that data logged (if it worked under that circumstance, or was it still an issue meaning the analysis or execution json needed to be modified), and then maybe different analysis suppositions in other cases imparted by the solutions team (this way they can A/B test solutions derived from grey areas seen in the definitive metadata, in order to get even more narrowed definitive metadata (regarding the action items, this would be a point of optimization where by identifying the specific points of failure, the system can either work around the points, work through them while looking out for them (if in uncharted territory that can't be certain if the issue will be evident there yet due to grey areas in the available metadata at that time), or develop other creative solutions to address the issue at its root (for example, maybe if an issue kept popping up with it not being able to access a particular website, then it would be determined that idk like maybe a passwords section was required for the user of the brain))..... I kind of touched on it in this paragraph but just to be clear, continue on with my reasons for developing the action items below -To build new workflows and optimize existing ones.... Just as a human approaching a new task would plan out roughly how they thought would be a decent way to execute the task, by having action item micro components, the brain could develop workflows for itself based on a sequence of LLM workflow development that give a base supposition, that is broken down from task to subtask to sub-sub task, to sub-sub-sub task, which is then filled in with known and defined action items, as well as any other micro component action items it need to come up with in order to fulfill the sub sub subtask or whatever... -Transfer of knowledge accross domains --> By having action items, the brain can learn and act much more efficiently, by transfering in multiple domains not just one -Having something tangible to train it with--> If desired, you could do a semi-manual mode where instead of the brain running off of its current best known version, you could introduce it new tasks, alter its proposed workflows of action items (even if only for it to learn the issues if doing whatever action items in a particular) you want to workflows the brain You can use higher categorical action items in order to give very specific instructions. You don't need to know the best way to do something you have a specific translator that takes the highest category -LLM interaction, and contextual optimization knowledge application in unknown domains: have a set of verbs that interacts with the set of nouns identified already in exisiting vector databases that LLMs use. LLMs have an incredible ability to identify nouns through their vector database, and as of now, they attempt to give themselves legs (ie. coding). What they don't have is a way of doing actions, learning from their mistakes, learning about themselves and their flaws, and addressing their own issues, and then learning about themselves when they are actually making actions.... by learning about themselves while making actions they not only get data about their mistakes but also get data about the context of particular situations. .... obviously it looks a little different the way it plays out in the system, but to anthropomorphize the process, it lets the brain do this "hmmm I think this might be a good set of actions to in order to complete this task" and then fumbles through (collecting data from mistakes too), and can then say "wait a minuete thats not working because of XYZ, and its not working because of XYZ action items I'm trying to take"... I know I also do those same action items in BZA actions, so I should put a watch on those other actions to test and see if it happens there to get a more clear understanding of the issue"... and what that does is not only let it optimize but also gain contextual understandings (X is seen while doing this and this and this actions, but isn't seen in these actions even though I'm still doing X because the circumstances of doing Y action negates the issues seen in X action most of the time)... It allows it to see the differences in acting in one situation vs another that it can use to design unknown task action item workflows by applying its contextual knowledge of action flow errror differences, and metadata logs to the developement of strategic action workflows in another unknown area.
